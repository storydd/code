{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "import openpyxl\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_excel(open('t.xlsx', 'rb'),sheet_name='最终数据集',usecols=[1,2,3,4,5])\n",
    "Y=pd.read_excel(open('t.xlsx', 'rb'),sheet_name='最终数据集', usecols=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.51309060e-03, 6.92956613e-01, 9.00893032e-01, 2.99035605e-01,\n",
       "        0.00000000e+00],\n",
       "       [1.72762148e-03, 7.61284971e-01, 8.18152035e-01, 2.87403338e-01,\n",
       "        0.00000000e+00],\n",
       "       [9.69722793e-03, 7.42928911e-01, 8.25046358e-01, 2.31613408e-01,\n",
       "        0.00000000e+00],\n",
       "       ...,\n",
       "       [9.49731426e-06, 7.01200396e-01, 7.15529465e-01, 7.88507970e-02,\n",
       "        0.00000000e+00],\n",
       "       [9.59948855e-06, 5.62205664e-01, 7.61000940e-01, 1.08321132e-01,\n",
       "        0.00000000e+00],\n",
       "       [5.49939083e-05, 8.65418149e-01, 7.96174257e-01, 9.36803535e-02,\n",
       "        0.00000000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "mm = MinMaxScaler()\n",
    "X = mm.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1242358 ],\n",
       "       [0.03740689],\n",
       "       [0.40791365],\n",
       "       ...,\n",
       "       [0.02822565],\n",
       "       [0.01971066],\n",
       "       [0.3388823 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = mm.fit_transform(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3175, 1059, 3175, 1059)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_test),len(Y_train),len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_train\n",
    "Y=Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3175, 3175)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X),len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.01892523095011711\n",
      "Score for fold 2: loss of 0.023499848321080208\n",
      "Score for fold 3: loss of 0.021080011501908302\n",
      "Score for fold 4: loss of 0.02161267399787903\n",
      "Score for fold 5: loss of 0.022842036560177803\n",
      "Score for fold 6: loss of 0.020630206912755966\n",
      "Score for fold 7: loss of 0.022175606340169907\n",
      "Score for fold 8: loss of 0.02284662425518036\n",
      "Score for fold 9: loss of 0.026432620361447334\n",
      "Score for fold 10: loss of 0.0241570882499218\n",
      "average_score: 0.022420194745063782\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "fold_no = 1\n",
    "average_score = 0\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(4,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}')\n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 6ms/step - loss: 0.0430 - val_loss: 0.0302\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0262\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0256\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0254\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0252\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0253\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0251\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0253\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0251\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0256\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0253\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0255\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0251\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0256\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0251\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0253\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0254\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0259\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0258\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0251\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0259\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0251\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0253\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0254\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0251\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0251\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0253\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0254\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0258\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0253\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0253\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0253\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0261\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0257\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0253\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0251\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0262\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0251\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0253\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0254\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0255\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0253\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0251\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0255\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0255\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0253\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0260\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0254\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0254\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0253\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0252\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import adam_v2\n",
    "model=Sequential() #初始化模型\n",
    "model.add(Dense(32,input_shape=(5,),activation='tanh'))#添加一个隐含层，注：只是第一个隐含层需指定input_dim\n",
    "model.add(Dense(16,activation='tanh'))\n",
    "model.add(Dense(8,activation='tanh'))\n",
    "model.add(Dense(4,activation='tanh'))\n",
    "model.add(Dense(1)) #添加输出层\n",
    "model.compile(loss='mse', optimizer='adam')  # 编译，指定目标函数与优化方法\n",
    "history = model.fit(X_train,Y_train,epochs=100,validation_data=(X_test,Y_test),batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('loss.txt',loss,fmt='%f')\n",
    "np.savetxt('val_loss.txt',val_loss,fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('Epochs', fontsize = 12)\n",
    "pyplot.ylabel('Loss', fontsize = 12)\n",
    "pyplot.savefig(\"./loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pre=mm.inverse_transform(prediction_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real=mm.inverse_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cha=Y_pre-Y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 1.054\n",
      "Test MSE: 2.059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(Y_real, Y_pre)\n",
    "mse = mean_squared_error(Y_real, Y_pre)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test MSE: %.3f' % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.435\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(Y_real, Y_pre))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(Y_real,Y_pre):#计算mape\n",
    "    return np.mean(np.abs((Y_real - Y_pre) / Y_real)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape=MAPE(Y_real, Y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374.9015721408781"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.019407425075769424\n",
      "Score for fold 2: loss of 0.02353452891111374\n",
      "Score for fold 3: loss of 0.021221529692411423\n",
      "Score for fold 4: loss of 0.021424032747745514\n",
      "Score for fold 5: loss of 0.023078657686710358\n",
      "Score for fold 6: loss of 0.020592575892806053\n",
      "Score for fold 7: loss of 0.02283821441233158\n",
      "Score for fold 8: loss of 0.022705266252160072\n",
      "Score for fold 9: loss of 0.026860138401389122\n",
      "Score for fold 10: loss of 0.024046458303928375\n",
      "average_score: 0.022570882737636567\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.019006749615073204\n",
      "Score for fold 2: loss of 0.024016454815864563\n",
      "Score for fold 3: loss of 0.02143840119242668\n",
      "Score for fold 4: loss of 0.021376892924308777\n",
      "Score for fold 5: loss of 0.02288568764925003\n",
      "Score for fold 6: loss of 0.02071939967572689\n",
      "Score for fold 7: loss of 0.022322535514831543\n",
      "Score for fold 8: loss of 0.022837631404399872\n",
      "Score for fold 9: loss of 0.026804903522133827\n",
      "Score for fold 10: loss of 0.024694224819540977\n",
      "average_score: 0.022610288113355637\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(16,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.019128622487187386\n",
      "Score for fold 2: loss of 0.023645853623747826\n",
      "Score for fold 3: loss of 0.0209127776324749\n",
      "Score for fold 4: loss of 0.021620282903313637\n",
      "Score for fold 5: loss of 0.02341899648308754\n",
      "Score for fold 6: loss of 0.020603926852345467\n",
      "Score for fold 7: loss of 0.022236015647649765\n",
      "Score for fold 8: loss of 0.02295037917792797\n",
      "Score for fold 9: loss of 0.02671905793249607\n",
      "Score for fold 10: loss of 0.026247205212712288\n",
      "average_score: 0.022748311795294286\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import adam_v2\n",
    "\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(4,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    adam = optimizers.adam_v2.Adam(lr=0.0001)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.018839877098798752\n",
      "Score for fold 2: loss of 0.02384530007839203\n",
      "Score for fold 3: loss of 0.020982468500733376\n",
      "Score for fold 4: loss of 0.021452276036143303\n",
      "Score for fold 5: loss of 0.02300938405096531\n",
      "Score for fold 6: loss of 0.020739024505019188\n",
      "Score for fold 7: loss of 0.022146150469779968\n",
      "Score for fold 8: loss of 0.023190705105662346\n",
      "Score for fold 9: loss of 0.02674025483429432\n",
      "Score for fold 10: loss of 0.024442508816719055\n",
      "average_score: 0.022538794949650765\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import adam_v2\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    adam = optimizers.adam_v2.Adam(lr=0.0001)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.01884976029396057\n",
      "Score for fold 2: loss of 0.02388474904000759\n",
      "Score for fold 3: loss of 0.021149827167391777\n",
      "Score for fold 4: loss of 0.021748069673776627\n",
      "Score for fold 5: loss of 0.022698499262332916\n",
      "Score for fold 6: loss of 0.02049705944955349\n",
      "Score for fold 7: loss of 0.022254785522818565\n",
      "Score for fold 8: loss of 0.02267690934240818\n",
      "Score for fold 9: loss of 0.026957908645272255\n",
      "Score for fold 10: loss of 0.02405204065144062\n",
      "average_score: 0.02247696090489626\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import adam_v2\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(16,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    adam = optimizers.adam_v2.Adam(lr=0.0001)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.019132710993289948\n",
      "Score for fold 2: loss of 0.024468302726745605\n",
      "Score for fold 3: loss of 0.02117164246737957\n",
      "Score for fold 4: loss of 0.02187878079712391\n",
      "Score for fold 5: loss of 0.02273283340036869\n",
      "Score for fold 6: loss of 0.020609354600310326\n",
      "Score for fold 7: loss of 0.022474676370620728\n",
      "Score for fold 8: loss of 0.022978883236646652\n",
      "Score for fold 9: loss of 0.026985954493284225\n",
      "Score for fold 10: loss of 0.024383824318647385\n",
      "average_score: 0.022681696340441704\n"
     ]
    }
   ],
   "source": [
    "#7 SGD优化器\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(4,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    sgd = optimizers.gradient_descent_v2.SGD(lr=0.001)\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.019164102151989937\n",
      "Score for fold 2: loss of 0.02387324348092079\n",
      "Score for fold 3: loss of 0.02105099707841873\n",
      "Score for fold 4: loss of 0.021607989445328712\n",
      "Score for fold 5: loss of 0.022925741970539093\n",
      "Score for fold 6: loss of 0.020933158695697784\n",
      "Score for fold 7: loss of 0.022470176219940186\n",
      "Score for fold 8: loss of 0.022429799661040306\n",
      "Score for fold 9: loss of 0.027176562696695328\n",
      "Score for fold 10: loss of 0.02432657778263092\n",
      "average_score: 0.02259583491832018\n"
     ]
    }
   ],
   "source": [
    "#8 SGD优化器\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    sgd = optimizers.gradient_descent_v2.SGD(lr=0.001)\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100,verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.019043980166316032\n",
      "Score for fold 2: loss of 0.024135278537869453\n",
      "Score for fold 3: loss of 0.02110852487385273\n",
      "Score for fold 4: loss of 0.022155292332172394\n",
      "Score for fold 5: loss of 0.022704584524035454\n",
      "Score for fold 6: loss of 0.020755000412464142\n",
      "Score for fold 7: loss of 0.02283814549446106\n",
      "Score for fold 8: loss of 0.022153232246637344\n",
      "Score for fold 9: loss of 0.02700521983206272\n",
      "Score for fold 10: loss of 0.024363141506910324\n",
      "average_score: 0.022626239992678165\n"
     ]
    }
   ],
   "source": [
    "#9 SGD优化器\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(16,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    sgd = optimizers.gradient_descent_v2.SGD(lr=0.001)\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.019518332555890083\n",
      "Score for fold 2: loss of 0.02438138984143734\n",
      "Score for fold 3: loss of 0.020961634814739227\n",
      "Score for fold 4: loss of 0.021686721593141556\n",
      "Score for fold 5: loss of 0.02286076918244362\n",
      "Score for fold 6: loss of 0.020522955805063248\n",
      "Score for fold 7: loss of 0.022441381588578224\n",
      "Score for fold 8: loss of 0.02255587838590145\n",
      "Score for fold 9: loss of 0.02696322835981846\n",
      "Score for fold 10: loss of 0.024591926485300064\n",
      "average_score: 0.022648421861231328\n"
     ]
    }
   ],
   "source": [
    "#10 SGD优化器\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(4,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    sgd = optimizers.gradient_descent_v2.SGD(lr=0.0001)\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.018932243809103966\n",
      "Score for fold 2: loss of 0.023941397666931152\n",
      "Score for fold 3: loss of 0.020998766645789146\n",
      "Score for fold 4: loss of 0.021800650283694267\n",
      "Score for fold 5: loss of 0.023233041167259216\n",
      "Score for fold 6: loss of 0.02060151845216751\n",
      "Score for fold 7: loss of 0.022555941715836525\n",
      "Score for fold 8: loss of 0.022667618468403816\n",
      "Score for fold 9: loss of 0.027585001662373543\n",
      "Score for fold 10: loss of 0.024514714255928993\n",
      "average_score: 0.022683089412748814\n"
     ]
    }
   ],
   "source": [
    "#11 SGD优化器\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(32,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(16,activation='tanh'))\n",
    "    model.add(Dense(4,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 0.01919964887201786\n",
      "Score for fold 2: loss of 0.024201905354857445\n",
      "Score for fold 3: loss of 0.021305741742253304\n",
      "Score for fold 4: loss of 0.021575169637799263\n",
      "Score for fold 5: loss of 0.02279576286673546\n",
      "Score for fold 6: loss of 0.020529568195343018\n",
      "Score for fold 7: loss of 0.022510569542646408\n",
      "Score for fold 8: loss of 0.02251635491847992\n",
      "Score for fold 9: loss of 0.026954278349876404\n",
      "Score for fold 10: loss of 0.025057382881641388\n",
      "average_score: 0.022664638236165047\n"
     ]
    }
   ],
   "source": [
    "#12 SGD优化器\n",
    "from keras import optimizers\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# 定义10折交叉验证\n",
    "kf = KFold(n_splits=10)\n",
    "average_score = 0\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, Y):\n",
    "\n",
    "    # 在每次的循环中，都需要重新构建模型\n",
    "    model = Sequential() \n",
    "    model.add(Dense(16,input_shape=(5,),activation='tanh'))\n",
    "    model.add(Dense(8,activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    sgd = optimizers.gradient_descent_v2.SGD(lr=0.0001)\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X[train], Y[train], epochs=100, verbose=0)  # 根据情况调整epochs的数量\n",
    "\n",
    "    # 评估模型\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores}') \n",
    "\n",
    "    average_score += scores\n",
    "\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print('average_score:', average_score/(fold_no-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
